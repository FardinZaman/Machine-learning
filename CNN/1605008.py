# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193mM7D5p43aqpTRNfewIOPAk2IdW2ktG
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/MyDrive/CNN/

import sys
import numpy as np 
# import matplotlib.pyplot as plt 
from tensorflow.keras.datasets import mnist
from sklearn.metrics import f1_score , accuracy_score

np.set_printoptions(threshold=sys.maxsize)

(x_train , y_train) , (x_test , y_test) = mnist.load_data()

(x_train , y_train) , (x_test , y_test) = mnist.load_data()

print(x_train.shape)

x_train = np.expand_dims(x_train , axis=1)
x_test = np.expand_dims(x_test , axis=1)

print(x_train.shape)

# x_train = x_train[0:10000]
# y_train = y_train[0:10000]

x_validate = x_test[0:int(x_test.shape[0]/2)]
y_validate = y_test[0:int(y_test.shape[0]/2)]

x_test = x_test[int(x_test.shape[0]/2):]
y_test = y_test[int(y_test.shape[0]/2):]








# x_train = x_train[0:10000]
# y_train = y_train[0:10000]

# x_test = x_test[0:200]
# y_test = y_test[0:200]

# def unpickle(file):
#   import pickle
#   with open(file, 'rb') as fo:
#     dict = pickle.load(fo, encoding='latin1')
#   return dict



# x_train = []
# y_train = []

# for b in range(1,6):
#   f = "cifar-10-batches-py/data_batch_" + str(b)
#   dict1 = unpickle(f)

#   x = np.array(dict1['data'].reshape(10000, 3, 32, 32))
#   y = np.array(dict1['labels']).reshape(10000, 1)

#   print((y.shape))

#   x_train.append(x)
#   y_train.append(y)

# x_train = np.array(x_train).reshape(50000, 3, 32, 32)
# y_train = np.array(y_train).reshape(50000, 1)

# dict2 = unpickle("cifar-10-batches-py/test_batch")

# x_test = np.array(dict2['data'].reshape(10000, 3, 32, 32))
# y_test = np.array(dict2['labels']).reshape(10000, 1)

# # x_train = x_train[0:10000]
# # y_train = y_train[0:10000]

# # x_test = x_test[0:2000]
# # y_test = y_test[0:2000]


# x_validate = x_test[0:int(x_test.shape[0]/2)]
# y_validate = y_test[0:int(y_test.shape[0]/2)]

# x_test = x_test[int(x_test.shape[0]/2):]
# y_test = y_test[int(y_test.shape[0]/2):]






# x_validate = x_test[0:5000]
# y_validate = y_test[0:5000]

# x_test = x_test[5000:]
# y_test = y_test[5000:]

# f = "cifar-10-batches-py/data_batch_1"
# dict = unpickle(f)

# x_train = np.array(dict['data']).reshape(10000, 3, 32, 32)
# y_train = np.array(dict['labels']).reshape(10000, 1)

# x_train.shape
# y_train.shape

# x_train = x_train[4000:5000]
# y_train = y_train[4000:5000]

# x_test = x_test[3000:4000]
# y_test = y_test[3000:4000]

# x_train = np.delete(x_train , 1 , axis=1)
# x_test = np.delete(x_test , 1 , axis=1)

# x_train = np.delete(x_train , 1 , axis=1)
# x_test = np.delete(x_test , 1 , axis=1)

# x_train = x_train / 255
# x_validate = x_validate / 255
# x_test = x_test / 255

# x_train = x_train / 255
# x_test = x_test / 255

# print(x_train[0])
# print(x_train[0].reshape(3,32,32))


print(x_train.shape)
print(x_validate.shape)
print(x_test.shape)

def create_batches(x, y, batch_size):
  
  batches = [] 
  
  number_of_examples = x.shape[0]
  number_of_batches = number_of_examples // batch_size 
  
  i = 0
  for i in range(number_of_batches):
    x_little = x[i*batch_size:(i+1)*batch_size , :]
    y_little = y[i*batch_size:(i+1)*batch_size]
    batches.append((x_little , y_little))
  
  if number_of_examples % batch_size != 0:
    x_little = x[i*batch_size: , :]
    y_little = y[i*batch_size:]
    batches.append((x_little , y_little))
  
  return batches

def convolution_forward(x , weight , bias , stride , padding):
  
  N , Channel , Height , Width = x.shape 
  Filter_number , Filter_depth , Filter_height , Filter_width = weight.shape

  Width_after_filter = int((Width - Filter_width + 2*padding)/stride + 1)
  Height_after_filter = int((Height - Filter_height + 2*padding)/stride + 1)

  x_after_filter = np.zeros((N , Filter_number , Height_after_filter , Width_after_filter))

  x_after_padding = np.pad(x , ((0 , 0) , (0 , 0) , (padding , padding) , (padding , padding)) , mode='constant')
  _, _, Height_after_padding , Width_after_padding = x_after_padding.shape

  weight_rows = weight.reshape(Filter_number , Channel * Filter_height * Filter_width)
  
  x_columns = np.zeros((Channel * Filter_height * Filter_width , Height_after_filter * Width_after_filter))

  for i in range(N):
    c = 0
    for j in range(0 , Height_after_padding - Filter_height + 1 , stride):
      for k in range(0 , Width_after_padding - Filter_width + 1 , stride):
        x_columns[: , c] = x_after_padding[i , : , j:j+Filter_height , k:k+Filter_width].reshape(Channel * Filter_height * Filter_width)
        c += 1
    x_after_filter[i , : , : , :] = (np.dot(weight_rows , x_columns) + bias.reshape(-1 , 1)).reshape(Filter_number , Height_after_filter , Width_after_filter)

  cache = (x , weight , bias , stride, padding)
  
  return x_after_filter , cache

def convolution_backward(dout , cache):
  x , weight , bias , stride , padding = cache 
  
  N , Channel , Height , Width = x.shape 
  Filter_number , Filter_depth , Filter_height , Filter_width = weight.shape 
  _, _, Height_of_dout , Width_of_dout = dout.shape 

  x_after_padding = np.pad(x , ((0 , 0) , (0 , 0) , (padding , padding) , (padding , padding)) , mode='constant')
  _, _, Height_after_padding , Width_after_padding = x_after_padding.shape
  
  weight_rows = weight.reshape(Filter_number, Channel * Filter_height * Filter_width)

  x_columns = np.zeros((Channel * Filter_height * Filter_width , Height_of_dout * Width_of_dout))
  
  dx = np.zeros_like(x)
  dw = np.zeros_like(weight)
  db = np.zeros_like(bias) 

  for i in range(N):
    one_image = dout[i , : , : , :].reshape(Filter_number , Height_of_dout * Width_of_dout)
    dout_weighted = np.dot(weight_rows.T , one_image)
    d_x_after_padding = np.zeros(x_after_padding.shape[1:])
    c = 0
    for j in range(0 , Height_after_padding - Filter_height + 1 , stride):
      for k in range(0 , Width_after_padding - Filter_width + 1 , stride):
        d_x_after_padding[: , j:j+Filter_height , k:k+Filter_width] += dout_weighted[: , c].reshape(Channel , Filter_height , Filter_width)
        x_columns[: , c] = x_after_padding[i , : , j:j+Filter_height , k:k+Filter_width].reshape(Channel * Filter_height * Filter_width) 
        c += 1
    
    if padding == 0:
      dx[i] = d_x_after_padding
    else:
      dx[i] = d_x_after_padding[: , padding:-padding , padding:-padding]
    
    dw += np.dot(one_image , x_columns.T).reshape(Filter_number , Channel , Filter_height , Filter_width)
    db += np.sum(one_image , axis=1)

  # dw = dw / N
  # db = db / N
  # print(dw[0][0][0])
  # print(dout[0][0][0])

  return dx, dw, db

def ReLU_forward(x):
  
  x_after_ReLU = np.maximum(x , 0)
  
  cache = x

  return x_after_ReLU , cache

def ReLU_backward(dout , cache):
   
  x = cache
  
  dx = dout * (x > 0)

  return dx

def pooling_forward(x , Pool_height , Pool_width , stride):

  N , Channel , Height , Width = x.shape

  Height_after_pool = 1 + (Height - Pool_height) // stride
  Width_after_pool = 1 + (Width - Pool_width) // stride 

  x_after_pool = np.zeros((N , Channel , Height_after_pool , Width_after_pool))

  for i in range(N):
    one_image = np.zeros((Channel , Height_after_pool * Width_after_pool))
    c = 0
    for j in range(0 , Height - Pool_height + 1 , stride):
      for k in range(0 , Width - Pool_width + 1 , stride):
        one_box = x[i , : , j:j+Pool_height , k:k+Pool_width].reshape(Channel , Pool_height * Pool_width)
        max_in_box = np.max(one_box , axis=1)
        one_image[: , c] = max_in_box
        c += 1
    x_after_pool[i, :, :, :] = one_image.reshape(Channel , Height_after_pool , Width_after_pool)

  cache = (x , Pool_height , Pool_width , stride)
  
  return x_after_pool, cache

def pooling_backward(dout , cache):
  
  x , Pool_height , Pool_width , stride = cache 
  N , Channel , Height , Width = x.shape
  _, _, Height_of_dout , Width_of_dout = dout.shape 

  dx = np.zeros_like(x)

  for i in range(N):
    one_image = dout[i , :].reshape(Channel , Height_of_dout * Width_of_dout)
    c = 0
    for j in range(0, Height - Pool_height + 1, stride):
      for k in range(0, Width - Pool_width + 1, stride): 
        one_box = x[i, :, j:j+Pool_height, k:k+Pool_width].reshape(Channel , Pool_height * Pool_width)
        index_of_max_in_box = np.argmax(one_box , axis=1)
        values_from_dout = one_image[: , c]
        dpooling = np.zeros_like(one_box)
        dpooling[np.arange(Channel), index_of_max_in_box] = values_from_dout
        dx[i , : , j:j+Pool_height , k:k+Pool_height] = dpooling.reshape(Channel , Pool_height , Pool_width)
        c += 1
  
  return dx

def fully_connected_forward(x , weight , bias):
  
  N , Channel , Height , Width = x.shape
  
  x_after_flatten = x.reshape(N , -1)

  x_after_fully_connect = np.dot(x_after_flatten , weight) + bias
  
  cache = (x , weight , bias)
  
  return x_after_fully_connect , cache

def fully_connected_backward(dout , cache):
  
  # print(dout[0])

  x , weight , bias = cache 
  
  N = x.shape[0]
  x_reshaped = x.reshape(N, -1)

  dx = np.dot(dout , weight.T).reshape(x.shape)
  dw = np.dot(x_reshaped.T , dout)
  db = np.sum(dout.T , axis=1)

  return dx, dw, db

def softmax(x):
  
  # print("x : " , x[0])

  x = x - np.max(x , axis=1 , keepdims=True)
  # print("x : " , x[0])
  numerator = np.exp(x)
  probabilities = numerator / np.sum(numerator , axis=1 , keepdims=True)

  # print("num : " , numerator[0])

  return probabilities

def cross_entropy_loss(probabilities , y):
  
  N = probabilities.shape[0]
  
  # loss = -np.sum(np.log(probabilities[np.arange(N), y])) / N

  probs = probabilities[np.arange(N), y]
  probs[probs == 0] = 1e-323
  loss = -np.sum(np.log(probs)) / N

  derivative = probabilities.copy()
  derivative[np.arange(N), y] -= 1
  derivative /= N

  return loss, derivative

# Weight_conv_1 = np.random.normal(0.0, 0.01, (6, 1, 5, 5))
Weight_conv_1 = np.random.normal(0.0, 0.01, (6, 3, 5, 5))
Weight_conv_1 = Weight_conv_1 * 1e-10
bias_conv_1 = np.zeros((6, ))

Weight_conv_2 = np.random.normal(0.0, 0.01, (12, 6, 5, 5))
Weight_conv_2 = Weight_conv_2 * 1e-10
bias_conv_2 = np.zeros((12, ))

Weight_conv_3 = np.random.normal(0.0, 0.01, (100, 12, 5, 5))
Weight_conv_3 = Weight_conv_3 * 1e-10
bias_conv_3 = np.zeros((100, ))

# Weight_fc = np.random.normal(0.0, 0.01, (100, 10))
Weight_fc = np.random.normal(0.0, 0.01, (100*2*2, 10))
Weight_fc = Weight_fc * 1e-5
bias_fc = np.zeros((10,))

def forward_and_backward_phase(x , y , evaluate , validate):
  
  global Weight_conv_1
  global Weight_conv_2
  global Weight_conv_3
  global Weight_fc
  global bias_conv_1
  global bias_conv_2
  global bias_conv_3
  global bias_fc

  # print(Weight_conv_1.shape)
  # print(x[0][0][0])
  x1 , Cache_conv_1 = convolution_forward(x , Weight_conv_1 , bias_conv_1 , 1 , 2)

  x2 , Cache_ReLU_1 = ReLU_forward(x1)

  x3 , Cache_pool_1 = pooling_forward(x2 , 2 , 2 , 2)

  x4 , Cache_conv_2 = convolution_forward(x3,  Weight_conv_2 , bias_conv_2 , 1 , 0)

  x5 , Cache_ReLU_2 = ReLU_forward(x4)

  x6 , Cache_pool_2 = pooling_forward(x5 , 2 , 2 , 2)

  x7 , Cache_conv_3 = convolution_forward(x6,  Weight_conv_3 , bias_conv_3 , 1 , 0)

  x8 , Cache_ReLU_3 = ReLU_forward(x7)

  x9 , Cache_fc = fully_connected_forward(x8 , Weight_fc , bias_fc)

  probabilities = softmax(x9)

  if(evaluate == True):
    return probabilities

  loss , dx9 = cross_entropy_loss(probabilities , y)
  # print("dx9 : ")
  # print(dx9[0])
  if(validate == True):
    return probabilities , loss

  dx8 , dw4 , db4 = fully_connected_backward(dx9 , Cache_fc)

  dx7 = ReLU_backward(dx8 , Cache_ReLU_3)

  dx6 , dw3 , db3 = convolution_backward(dx7 , Cache_conv_3)

  dx5 = pooling_backward(dx6 , Cache_pool_2)

  dx4 = ReLU_backward(dx5 , Cache_ReLU_2)

  dx3 , dw2 , db2 = convolution_backward(dx4 , Cache_conv_2)

  dx2 = pooling_backward(dx3 , Cache_pool_1)

  dx1 = ReLU_backward(dx2 , Cache_ReLU_1)

  dx0 , dw1 , db1 = convolution_backward(dx1 , Cache_conv_1)
  # print(dw1.shape)

  gradients = {
    'Weight_conv_1' : dw1,
    'Weight_conv_2' : dw2,
    'Weight_conv_3' : dw3,
    'Weight_fc' : dw4,
    'bias_conv_1' : db1,
    'bias_conv_2' : db2,
    'bias_conv_3' : db3,
    'bias_fc' : db4,
  }

  return gradients, loss

def train(epochs , learning_rate , batches):
  
  global Weight_conv_1
  global Weight_conv_2
  global Weight_conv_3
  global Weight_fc
  global bias_conv_1
  global bias_conv_2
  global bias_conv_3
  global bias_fc

  loss_container = []
  for e in range(epochs):
    # loss_container = []

    print("epoch : " , e+1)
    for b in batches:
      x , y = b
      gradients , loss = forward_and_backward_phase(x , y , False , False)

      Weight_conv_1 -= learning_rate * gradients['Weight_conv_1']
      Weight_conv_2 -= learning_rate * gradients['Weight_conv_2']
      Weight_conv_3 -= learning_rate * gradients['Weight_conv_3']
      Weight_fc -= learning_rate * gradients['Weight_fc']
      bias_conv_1 -= learning_rate * gradients['bias_conv_1']
      bias_conv_2 -= learning_rate * gradients['bias_conv_2']
      bias_conv_3 -= learning_rate * gradients['bias_conv_3']
      bias_fc -= learning_rate * gradients['bias_fc']

      # print("CW : ")
      # print(gradients['Weight_conv_1'][0])

    val_loss , accuracy , macro_f1_score = validation(x_validate , y_validate)
    
    loss_container.append(val_loss)

    print("validation loss : " , val_loss)
    print("Accuracy : " , accuracy)
    print("Macro-f1-score : " , macro_f1_score)

      # print("CW : ")
      # print(gradients['Weight_fc'][0])

  return loss_container

def from_file():
  
  f = open('input.txt')
  layers = []

  convolution_parameters = []
  pool_parameters = []
  fc_parameters = []

  for line in f:
    words = line.split(' ')
    words[-1] = words[-1].split('\n')[0]

    layers.append(words[0])

    if(words[0] == 'Conv'):
      convolution_parameters.append((int(words[1]), int(words[2]), int(words[3]), int(words[4])))

    if(words[0] == 'Pool'):
      pool_parameters.append((int(words[1]), int(words[2])))

    if(words[0] == 'FC'):
      fc_parameters.append((int(words[1])))

    if(words[0] == 'ReLU'):
      print("ReLU : No parameters")

    if(words[0] == 'Softmax'):
      print("Softmax : No parameters")

  return layers , convolution_parameters , pool_parameters , fc_parameters

first_iteration = True
Conv_Weights = []
Conv_biases = []
FC_Weights = []
FC_biases = []

layers = None
convolution_parameters = None
pool_parameters = None
fc_parameters = None

layers , convolution_parameters , pool_parameters , fc_parameters = from_file()


Best_CW = []
Best_Cb = []
Best_FCW = []
Best_FCb = []
best_f1 = 0

def fb_phase_with_input(x , y , evaluate , validate):
    
  global first_iteration
  global Conv_Weights
  global Conv_biases
  global FC_Weights
  global FC_biases

  global layers
  global convolution_parameter
  global pool_parameters
  global fc_parameters
  
  if first_iteration:
    # layers , convolution_parameters , pool_parameters , fc_parameters = from_file()
    print(layers)
  
  iterator = 0
  convolution_iterator = 0
  ReLU_iterator = 0
  pool_iterator = 0
  fc_iterator = 0
  

  x_out = []
  x_out.append(x)
  
  Cache_conv = []
  Cache_ReLU = []
  Cache_pool = []
  Cache_fc = []

  probs = []

  for layer in layers:

    if(layer == 'Conv'):
      _ , C , _ , _ = x_out[iterator].shape
      F , FD , stride , padding = convolution_parameters[convolution_iterator]
      
      if first_iteration:
        # Conv_Weights.append(np.random.normal(0.0 , 0.01 , (F, C, FD, FD)))
        Conv_Weights.append(np.random.normal(0.0 , 0.01 , (F, C, FD, FD)) * 1e-20)
        Conv_biases.append(np.zeros((F , )))

      out , cache = convolution_forward(x_out[iterator] , Conv_Weights[convolution_iterator] , Conv_biases[convolution_iterator] , stride , padding)
      x_out.append(out)
      Cache_conv.append(cache)

      convolution_iterator += 1

    if(layer == 'ReLU'):
      out , cache = ReLU_forward(x_out[iterator])
      x_out.append(out)
      Cache_ReLU.append(cache)
      
      ReLU_iterator += 1

    if(layer == 'Pool'):
      PD , stride = pool_parameters[pool_iterator]
      out , cache = pooling_forward(x_out[iterator] , PD , PD , stride)
      x_out.append(out)
      Cache_pool.append(cache)

      pool_iterator += 1

    if(layer == 'FC'):
      _ , C , H , W = x_out[iterator].shape
      if first_iteration:
        # FC_Weights.append(np.random.normal(0.0 , 0.01 , (C*H*W , fc_parameters[fc_iterator])))
        FC_Weights.append(np.random.normal(0.0 , 0.01 , (C*H*W , fc_parameters[fc_iterator])) * 1e-10)
        FC_biases.append(np.zeros((fc_parameters[fc_iterator] , )))

      out , cache = fully_connected_forward(x_out[iterator] , FC_Weights[fc_iterator], FC_biases[fc_iterator])
      x_out.append(out)
      Cache_fc.append(cache)

      fc_iterator += 1

    if(layer == 'Softmax'):
        probabilities = softmax(x_out[iterator])

    iterator += 1


  if(evaluate == True):
      return probabilities

  
  
  dx = []
  dW_conv = []
  db_conv = []
  dW_fc = []
  db_fc = []

  iterator = -1
  convolution_iterator -= 1
  ReLU_iterator -= 1
  pool_iterator -= 1
  fc_iterator -= 1

  loss = None
  

  for layer in reversed(layers):

    if(layer == 'Softmax'):
      loss , dout =  cross_entropy_loss(probabilities , y)
      dx.append(dout)

      if(validate == True):
        return probabilities , loss

    if(layer == 'FC'):
      dout, dW, db, = fully_connected_backward(dx[iterator] , Cache_fc[fc_iterator])
      dx.append(dout)
      dW_fc.insert(0 , dW)
      db_fc.insert(0 , db)

      fc_iterator -= 1

    if(layer == 'ReLU'):
      dout = ReLU_backward(dx[iterator] , Cache_ReLU[ReLU_iterator])
      dx.append(dout)

      ReLU_iterator -= 1

    if(layer == 'Conv'):
      dout, dW, db = convolution_backward(dx[iterator] , Cache_conv[convolution_iterator])
      dx.append(dout)
      dW_conv.insert(0, dW)
      db_conv.insert(0, db)

      convolution_iterator -= 1

    if(layer == 'Pool'):
      dx_i = pooling_backward(dx[iterator] , Cache_pool[pool_iterator])
      dx.append(dx_i)

      pool_iterator -= 1


    iterator += 1

  first_iteration = False

  gradients = {
      'W_conv' : dW_conv,
      'b_conv' : db_conv,
      'W_fc' : dW_fc,
      'b_fc' : db_fc
  }

  return gradients , loss

def train_when_input(epochs , learning_rate , batches):
    
  global Conv_Weights
  global Conv_biases
  global FC_Weights
  global FC_biases

  global Best_CW
  global Best_Cb
  global Best_FCW
  global Best_FCb
  global best_f1

  loss_container = []

  for e in range(epochs):
    print("epoch" , e+1)
    for b in batches:
      x , y = b
      gradients , loss = fb_phase_with_input(x , y , False , False)

      for i in range(len(Conv_Weights)):
        Conv_Weights[i] -= learning_rate * gradients['W_conv'][i]
        Conv_biases[i] -= learning_rate * gradients['b_conv'][i]

      for i in range(len(FC_Weights)):
        FC_Weights[i] -= learning_rate * gradients['W_fc'][i]
        FC_biases[i] -= learning_rate * gradients['b_fc'][i]

      # print(gradients['W_fc'][0][0])
      
      # loss_container.append(loss)

    val_loss , accuracy , macro_f1_score = validation(x_validate , y_validate)
    
    loss_container.append(val_loss)

    print("validation loss : " , val_loss)
    print("Accuracy : " , accuracy)
    print("Macro-f1-score : " , macro_f1_score)

    if(best_f1 < macro_f1_score):
      Best_CW = Conv_Weights
      Best_Cb = Conv_biases
      Best_FCW = FC_Weights
      Best_FCb = FC_biases

      best_f1 = macro_f1_score

      # print(e , ": \n")
      # print("CW" , Conv_Weights)
      # print("Cb" , Conv_biases)

  return loss_container

def evaluation(x , y):
  
  global Conv_Weights
  global Conv_biases
  global FC_Weights
  global FC_biases

  global Best_CW
  global Best_Cb
  global Best_FCW
  global Best_FCb

  Conv_Weights = Best_CW
  Conv_biases = Best_Cb
  FC_Weights = Best_FCW
  FC_biases = Best_FCb
  
  # probabilities = forward_and_backward_phase(x , y , True , False)
  # probabilities = fb_phase_with_input(x , y , True , False)

  # probabilities , loss = forward_and_backward_phase(x , y , False , True)
  probabilities , loss = fb_phase_with_input(x , y , False , True)
  
  predictions = np.argmax(probabilities , axis=1)
  
  # accuracy = np.mean(predictions == y)
  accuracy = accuracy_score(y , predictions)
  macro_f1_score = f1_score(y , predictions , average='macro' , zero_division = 1)

  print("Test : ")
  print("loss : " , loss)
  print("Accuracy : " , accuracy)
  print("Macro-f1-score : " , macro_f1_score)

  return accuracy

def validation(x , y):
  
  # probabilities , loss = forward_and_backward_phase(x , y , False , True)
  probabilities , loss = fb_phase_with_input(x , y , False , True)
  
  predictions = np.argmax(probabilities , axis=1)
  
  accuracy = accuracy_score(y , predictions)
  macro_f1_score = f1_score(y , predictions , average='macro' , zero_division = 1)

  # accuracy2 = np.mean(predictions == y)
  # print("manual : " , accuracy2)

  return loss , accuracy , macro_f1_score

batch_size = 512
epochs = 5
learning_rate = 0.01

batches = create_batches(x_train , y_train , batch_size)

# losses = train(epochs , learning_rate , batches)
losses = train_when_input(epochs , learning_rate , batches)

accuracy = evaluation(x_test, y_test)

# print(accuracy)

# a = np.array([1,2,3,4]).reshape(1,4)
# print(a)
# b = np.array([1,2,4,5,2,3,5,2,4,5,1,3,5,2,3,1]).reshape(4,4)
# print(b)

# c = (a @ b.T).reshape(2,2)

# print(c)

# a = np.random.random((3,3,2))
# a

# a = np.array([4 , 5 , -np.inf])
# print(a)
# b = a * .5
# print(b)

# a = np.random.random((4,2,2,2))
# print(a)
# b = np.delete(a ,0, axis=1)
# print(b.shape)