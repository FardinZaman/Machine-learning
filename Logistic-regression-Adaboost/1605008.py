# -*- coding: utf-8 -*-
"""adaboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ou9UVt9lArRBb506DLzWnBWp06HDFQoW
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/AdaBoost/

#!pip install numpy scikit-learn pandas matplotlib

import numpy as np
np.random.seed(2021)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.utils import shuffle

def normalize(dataset):
  for i in dataset.columns:
    if(dataset[i].nunique() > 2):
      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())
  return dataset

def change_yes_no(x):
  if x =='Yes' or x == 'Male':
      return 1
  elif x == 'No' or x == 'Female' :
      return 0
  else:
      return x

def preprocess1():
  dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
  dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'] , errors='coerce')

  for i in range(len(dataset['tenure'])):
    if(dataset.loc[i , 'tenure'] == 0):
      probable_tenure = random.random()
      monthlyCharge = dataset.loc[i , ['MonthlyCharges']]
      dataset.at[i , ['TotalCharges']] = float(probable_tenure * monthlyCharge)

  dataset.drop(['customerID'] , inplace=True , axis='columns')

  object_col=[]
  num_col=[]
  for i in dataset.columns:
    if dataset[i].dtype == 'object':
      object_col.append(i)
    else:
      num_col.append(i)

  for i in object_col:
    if len(dataset[i].unique()) == 2:
      dataset[i] = dataset[i].map(change_yes_no)

  dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})
  
  dataset_dummies = pd.get_dummies(dataset)

  return dataset_dummies

def preprocess2():
  dataset = pd.read_csv('adult.csv')

  for i in dataset.columns:
    maximum_occured = dataset[i].value_counts().idxmax()
    dataset[i] = dataset[i].replace(' ?', maximum_occured)

  dataset[' Income'] = dataset[' Income'].map({' <=50K': -1, ' >50K': 1})

  for i in dataset.columns:
    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):
      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})

  dataset = pd.get_dummies(dataset)

  return dataset

def preprocess2_2():
  dataset = pd.read_csv('adult_test.csv')

  for i in dataset.columns:
    maximum_occured = dataset[i].value_counts().idxmax()
    dataset[i] = dataset[i].replace(' ?', maximum_occured)

  dataset[' Income'] = dataset[' Income'].map({' <=50K.': -1, ' >50K.': 1})

  for i in dataset.columns:
    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):
      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})

  dataset = pd.get_dummies(dataset)

  return dataset

def preprocess3():
  dataset = pd.read_csv('creditcard.csv')

  dataset['Class'] = dataset['Class'].map({1: 1, 0: -1})

  dataset_with_all_one = dataset[dataset['Class'] == 1]

  class_one_index = dataset_with_all_one.index

  dataset.drop(class_one_index)

  dataset_with_some_zero = dataset.sample(n=20000)

  dataset_merged = pd.concat([dataset_with_all_one, dataset_with_some_zero])

  dataset_merged = shuffle(dataset_merged)

  return dataset_merged

#dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

#dataset.head(12)

# dataset.info()

# dataset.isnull().sum()

# for i in dataset.columns:
#     print(f'{i} : {dataset[i].unique()} \n "{dataset[i].dtype}"')
#     print('-----------------------------------------')

# for i in dataset.columns:
#     print(f'unique values in column "{i}" is \n {dataset[i].value_counts()} ')
#     print('----------------------------------------------------------')

# dataset.shape

# dataset.duplicated().sum()

# dataset.describe()

#dataset.TotalCharges.dtype

# dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'] , errors='coerce')
# dataset.TotalCharges.dtype

#arr = []
# for i in range(len(dataset['tenure'])):
#   if(dataset.loc[i , 'tenure'] == 0):
#     probable_tenure = random.random()
#     #print(imagined_tenure)
#     #print(dataset.loc[i , ['MonthlyCharges']])
#     monthlyCharge = dataset.loc[i , ['MonthlyCharges']]
#     #print(the_loc)
#     dataset.at[i , ['TotalCharges']] = float(probable_tenure * monthlyCharge)
#     #print(dataset.loc[i , ['TotalCharges']])
#     #arr.append(i)
# #dataset.info()

#dataset.loc[100 ,['TotalCharges']]
# dataset.TotalCharges.dtype

# dataset.describe()

# dataset.columns

#dataset.drop(['customerID'] , inplace=True , axis='columns')

#dataset.columns

# plt.style.use('fivethirtyeight')

# object_col=[]
# num_col=[]
# for i in dataset.columns:
#     if dataset[i].dtype == 'object':
#         object_col.append(i)
#     else:
#         num_col.append(i)

#object_col

#num_col

# ax = sns.countplot(x=dataset.Churn)
# plt.show()

# bx = sns.countplot(x=dataset.Partner)
# plt.show()

# for i in num_col:
#     plt.figure(figsize=(7,10))
#     sns.boxplot(x=dataset.Churn, y=dataset[i], data=dataset , linewidth=1)
#     plt.show()

# dataset[['SeniorCitizen' , 'Churn']]

# cx = sns.countplot(x=dataset.SeniorCitizen , hue=dataset.Churn , data=dataset)
# plt.show()

# for i in object_col:
#     plt.figure(figsize=(7,10))
#     sns.countplot(x=dataset[i], hue=dataset.Churn, data=dataset , linewidth=1.0)
#     plt.show()

# def change_yes_no(x):
#     if x =='Yes' or x == 'Male':
#         return 1
#     elif x == 'No' or x == 'Female' :
#         return 0
#     else:
#         return x

# for i in object_col:
#     if len(dataset[i].unique()) == 2:
#         dataset[i] = dataset[i].map(change_yes_no)

#dataset_dummies = pd.get_dummies(dataset)

# for i in dataset_dummies.columns:
#     if(dataset_dummies[i].nunique() != 2):
#       dataset_dummies[i] = (dataset_dummies[i]-dataset_dummies[i].min())/(dataset_dummies[i].max()-dataset_dummies[i].min())

# matrix = dataset_dummies.to_numpy()
# matrix

dataset_dummies = preprocess1()
X_base = dataset_dummies.drop('Churn' , axis='columns')
y_base = dataset_dummies['Churn']

X_train, X_test, y_train, y_test = train_test_split(X_base , y_base , test_size = 0.2 , random_state=0)
X_train = normalize(X_train)
X_test = normalize(X_test)

X_train_array = X_train.values
X_test_array = X_test.values
y_train_array = y_train.values.reshape(-1,1)
y_test_array = y_test.values.reshape(-1,1)

# dataset_train = preprocess2()
# dataset_test = preprocess2_2()

# missing_columns = set( dataset_train.columns ) - set( dataset_test.columns )

# for c in missing_columns:
#   dataset_test[c] = 0

# dataset_test = dataset_test[dataset_train.columns]

# X_train = dataset_train.drop([' Income'] , axis='columns')
# y_train = dataset_train[' Income']

# X_test = dataset_test.drop([' Income'] , axis='columns')
# y_test = dataset_test[' Income']

# X_train = normalize(X_train)
# X_test = normalize(X_test)

# X_train_array = X_train.values
# X_test_array = X_test.values
# y_train_array = y_train.values.reshape(-1,1)
# y_test_array = y_test.values.reshape(-1,1)

# dataset = preprocess3()

# X_base = dataset.drop('Class' , axis='columns')
# y_base = dataset['Class']

# X_train, X_test, y_train, y_test = train_test_split(X_base , y_base , test_size = 0.2 , random_state=0)
# X_train = normalize(X_train)
# X_test = normalize(X_test)

# X_train_array = X_train.values
# X_test_array = X_test.values
# y_train_array = y_train.values.reshape(-1,1)
# y_test_array = y_test.values.reshape(-1,1)

y_test_array.shape

# def Tanh(z):
#     return float(math.tanh(z))

# def hypothesis(w,x):
#     z = 0
#     for i in range(len(w)):
#         xi = x[i]
#         z += np.dot(xi,w[i])
#     return Tanh(z)

# def cost_function_derivative(X_train,y_train,w,j,m,alpha):
#     sum = 0
#     for i in range(m):
#         xi = X_train.iloc[i]
#         hi = hypothesis(w,xi)
#         yt = y_train.iloc[i]
#         hit = np.subtract((float)(hi),float(yt))
#         error = np.dot(hit,X_train.iloc[i][j])
#         error = np.dot(error,(1-hi*hi))
#         sum += error
#     constant = float(alpha)/float(m)
#     j = constant * sum
#     return j

# def gradient_descent(X_train,y_train,w,m,alpha):
#     new_w = []
#     for j in range(len(w)):
#         new_w_value = w[j] - cost_function_derivative(X_train,y_train,w,j,m,alpha)
#         new_w.append(new_w_value)	
#     return new_w

# def logistic_regression(X_train,y_train,alpha,w,number_of_epoch):
#     m = len(y_train)
#     for x in range(number_of_epoch):
#         new_w = gradient_descent(X_train,y_train,w,m,alpha)
#         w = new_w
#         print ('weight ', w)	
            
#     score = 0
#     length = len(X_test)
#     for i in range(length):
#         prediction = round(hypothesis(w,X_test.iloc[i]))
#         answer = y_test.iloc[i]
#         if prediction == answer:
#             score += 1
#     final_score = float(score) / float(length)
#     print ('Score with tanh: ', final_score)

#     return w

def Tanh(z):
    #print(z)
    tan_h_done = np.tanh(z)
    #print(tan_h_done)
    return tan_h_done

def hypothesis(w , X):
    z = X @ w
    # print('z.shape',z.shape)
    return Tanh(z)

def mean_L2_loss(y , y_hat):
  difference = y - y_hat
  return np.mean(np.power(difference , 2))

def gradient_descent(X , y , y_hat , alpha):
    m = X.shape[0]
    #dw = (1/m) * ( X.T @ ((y - y_hat) * (1 - y_hat**2)) )
    difference = y - y_hat
    derivative = 1 - np.power(y_hat , 2)
    multiplied = difference * derivative
    # print('multi: ', multi.shape)
    # print('derivative: ', derivative.shape)
    # print('diff: ', diff.shape)
    #dw = np.dot(X.T , multi)
    dw = X.T @ multiplied
    # print('dw: ', dw.shape)
    constant = (1/float(m)) * float(alpha) 
    return constant * dw

def logistic_regression(X , y , number_of_iterations , alpha , boundary):
  m,n = X.shape
  w = np.zeros((n+1 , 1))
  # print(w.shape)
  X_train_now = np.concatenate((np.ones((m, 1)), X) , axis = 1)

  for i in range(number_of_iterations):
    y_hat = hypothesis(w , X_train_now)
    # print('y_hat' , y_hat.shape)
    decay = gradient_descent(X_train_now , y , y_hat , alpha)
    # print('decay :' , decay.shape)
    # print('w_before :' , w.shape)
    w = w + decay
    # print('w_after :' , w.shape)
    error = mean_L2_loss(y , y_hat)

    if error < boundary:
      #print('bingo', i)
      break

  match = 0
  length = X_test_array.shape[0]
  X_test_now = np.concatenate((np.ones((length, 1)), X_test_array), axis=1)
  
  predicted_labels = hypothesis(w, X_test_now)
  # for i in range(prediction.shape[0]):
  #   if(np.isnan(prediction[i].any())):
  #       print('decay')
  #print(predicted_labels)
  #print('pred uniq ', np.unique(predicted_labels, axis=0))
  
  for i in range(length):
    if predicted_labels[i][0] < 0:
      predicted_labels[i][0] = -1
    else:
      predicted_labels[i][0] = 1
    
    real_label = y_test_array[i][0]
    if predicted_labels[i][0] == real_label:
      match += 1

  #print('pred uniq ', np.unique(prediction, axis=0))
  accuracy = float(match) / float(length)
  print ('Accuracy: ', accuracy)
  
  return w

def test_logistic_regression():
  w = logistic_regression(X_train_array , y_train_array , iterations , alpha , 0.5)
  
  length = X_test_array.shape[0]
  X_test_now = np.concatenate((np.ones((length, 1)), X_test_array), axis=1)
  predicted_labels = hypothesis(w, X_test_now)
  for i in range(length):
    if predicted_labels[i][0] < 0:
      predicted_labels[i][0] = -1
    else:
      predicted_labels[i][0] = 1
  CM = confusion_matrix(y_test_array , predicted_labels)
  print(CM)
  TN = CM[0][0]
  FP = CM[0][1]
  FN = CM[1][0]
  TP = CM[1][1]
  #print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))
  #print(float(match) / float(m2))
  print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))
  sensitivity = TP/(TP+FN)
  print('sensitivity :' , sensitivity)
  specificity = TN/(TN+FP)
  print('specificity :' , specificity)
  precision = TP/(TP+FP)
  print('precision :' , precision)
  false_discovery_rate = 1 - precision
  print('false_discovery_rate :' , false_discovery_rate)
  f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))
  print('f1_score :' , f1_score)




  length = X_train_array.shape[0]
  X_test_now = np.concatenate((np.ones((length, 1)), X_train_array), axis=1)
  predicted_labels2 = hypothesis(w, X_test_now)
  for i in range(length):
    if predicted_labels2[i][0] < 0:
      predicted_labels2[i][0] = -1
    else:
      predicted_labels2[i][0] = 1
  CM = confusion_matrix(y_train_array , predicted_labels2)
  print(CM)
  TN = CM[0][0]
  FP = CM[0][1]
  FN = CM[1][0]
  TP = CM[1][1]
  #print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))
  #print(float(match) / float(m2))
  print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))
  sensitivity = TP/(TP+FN)
  print('sensitivity :' , sensitivity)
  specificity = TN/(TN+FP)
  print('specificity :' , specificity)
  precision = TP/(TP+FP)
  print('precision :' , precision)
  false_discovery_rate = 1 - precision
  print('false_discovery_rate :' , false_discovery_rate)
  f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))
  print('f1_score :' , f1_score)

#test_logistic_regression()

def weighted_majority(X , hc , z):
  m = X.shape[0]
  loops = len(z)
  
  y_cap = np.zeros(m)
  for k in range(loops):
    h = hypothesis(hc[k].reshape(-1, 1) , X)
    for i in range(m):
      multiply_by_z = h[i][0]*z[k]
      y_cap[i] = y_cap[i] + multiply_by_z

  return y_cap

def calculate_metrices(X_incoming , y_incoming , hc , z):
    X_test_final = np.concatenate((np.ones((X_incoming.shape[0], 1)), X_incoming), axis=1)
    y_cap = weighted_majority(X_test_final , hc , z)
    #print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))

    tp = 0
    tn = 0
    m2 = X_incoming.shape[0]
    match = 0
    for i in range(m2):
      if y_cap[i] < 0:
        y_cap[i] = -1
      else:
        y_cap[i] = 1

      original_label = y_incoming[i][0]
      if y_cap[i] == original_label:
        # if original_label == 1:
        #   tp = tp + 1
        # else:
        #   tn = tn + 1
        match += 1
    
    CM = confusion_matrix(y_incoming , y_cap)
    print(CM)
    #print(tp , tn)
    TN = CM[0][0]
    FP = CM[0][1]
    FN = CM[1][0]
    TP = CM[1][1]
    print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))
    print(float(match) / float(m2))
    print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))
    # sensitivity = TP/(TP+FN)
    # print('sensitivity :' , sensitivity)
    # specificity = TN/(TN+FP)
    # print('specificity :' , specificity)
    # precision = TP/(TP+FP)
    # print('precision :' , precision)
    # false_discovery_rate = 1 - precision
    # print('false_discovery_rate :' , false_discovery_rate)
    # f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))
    # print('f1_score :' , f1_score)

def AdaBoost(K):
  m = y_train_array.shape[0]

  weight = np.empty(m)
  weight.fill(1.0/float(m))
  
  hc = []
  z = []

  #np.random.seed(2021)

  X_train_for_sample = np.copy(X_train_array)
  y_train_for_sample = np.copy(y_train_array)

  for k in range(K):
    
    resampled_rows = np.random.choice(X_train_array.shape[0] , size=m , p=weight)
    #print(resampled_rows)
    X_train_for_sample = X_train_array[resampled_rows , :]
    y_train_for_sample = y_train_array[resampled_rows , :]
    #print(temp)
    
    w = logistic_regression(X_train_for_sample , y_train_for_sample , iterations , alpha , 0.5)

    error = 0

    # for j in range(m):
    #   xj = X_train_for_sample.iloc[j]
    #   hj = hypothesis(w,xj)
    #   yj = y_train_for_sample.iloc[j]
    #   if hj != yj:
    #     error = error + weight[j]
    
    
    X_train_for_sample_2 = np.concatenate((np.ones((m, 1)), X_train_for_sample), axis=1)
    h = hypothesis(w , X_train_for_sample_2)
    for j in range(m):
      if h[j][0] < 0:
        h[j][0] = -1
      else:
        h[j][0] = 1
      yj = y_train_for_sample[j][0]
      if h[j][0] != yj:
        error = error + weight[j]

    if error > 0.5:
      print('bingo')
      continue

    # for j in range(m):
    #   xj = X_train_for_sample.iloc[j]
    #   hj = hypothesis(w,xj)
    #   yj = y_train_for_sample.iloc[j]
    #   if hj == yj:
    #     weight[j] = weight[j] * float(error)/(1.0-float(error))

    h = hypothesis(w , X_train_for_sample_2)
    for j in range(m):
      if h[j][0] < 0:
        h[j][0] = -1
      else:
        h[j][0] = 1
      yj = y_train_for_sample[j][0]
      if h[j][0] == yj:
        weight[j] = weight[j] * float(error)/(1.0-float(error))

    weight = weight/np.sum(weight)

    hc.append(w)
    z.append(np.log2(  (1.0-float(error))   /   float(error)   ))

  calculate_metrices(X_test_array , y_test_array , hc , z)
  calculate_metrices(X_train_array , y_train_array , hc , z)
  # def calculate_metrices(X_incoming , y_incoming):
  #   X_test_final = np.concatenate((np.ones((X_test_array.shape[0], 1)), X_test_array), axis=1)
  #   y_cap = weighted_majority(X_test_final , hc , z)
  #   #print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))

  #   m2 = X_test_array.shape[0]
  #   match = 0
  #   for i in range(m2):
  #     if y_cap[i] < 0:
  #       y_cap[i] = -1
  #     else:
  #       y_cap[i] = 1

  #     original_label = y_test_array[i][0]
  #     if y_cap[i] == original_label:
  #       match += 1
    
  #   CM = confusion_matrix(y_test_array , y_cap)
  #   print(CM)
  #   TP = CM[0][0]
  #   FN = CM[0][1]
  #   FP = CM[1][0]
  #   TN = CM[1][1]
  #   print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))
  #   print(float(match) / float(m2))
  #   print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))
  #   sensitivity = TP/(TP+FN)
  #   print('sensitivity :' , sensitivity)
  #   specificity = TN/(TN+FP)
  #   print('specificity :' , specificity)
  #   precision = TP/(TP+FP)
  #   print('precision :' , precision)
  #   false_discovery_rate = 1 - precision
  #   print('false_discovery_rate :' , false_discovery_rate)
  #   f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))
  #   print('f1_score :' , f1_score)

# initial_w = np.zeros(X_test.iloc[1].shape)
# alpha = 0.05
# iterations = 10
# logistic_regression(alpha,initial_w,iterations)
#initial_w = np.zeros(X_test.iloc[1].shape)
alpha = 0.2
iterations = 100
AdaBoost(5)